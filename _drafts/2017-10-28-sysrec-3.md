---
layout: post
title:  "Sistemas recomendadores, parte 3"
date:   2017-10-29 20:01:00 -0600
categories: sysrec spanish
draft: true
---
<!-- entry 3, clase 25.10 -->


En la [parte 2]({{ site.baseurl }}{% post_url 2017-10-28-sysrec-2 %}) se vieron varios algunas formas de clasificar sistemas recomendadores (SR). El post cerró en el filtrado colaborativo en base a usuarios, y se mencionó el uso de KNN o *K Nearest Neighbors*, un método *basado en memoria*. En términos generales, se puede resumir que  

1.  La predicción del rating sobre un ítem $$i$$ mejora mientras más vecinos $$k$$ de un *pool* de $$n$$ elementos de dimensión $$d$$ se consideren en el cálculo de similitud.
1.  La complejidad de calcular la distancia a un ejemplo es de $$\mathcal{O}(d)$$
1.  La complejidad de calcular la distancia a $$n$$ ejemplos es de $$\mathcal{O}(dn)$$
1.  Una vez calculadas las $$n$$ distancias, la complejidad de recorrer los $$k$$ vecinos más cercanos es de $$\mathcal{O}(nk)$$
1.  El tiempo total entonces es de $$\mathcal{O}(nd+nk)$$, (o $$\mathcal{O}(ndk)$$ [en función de cómo se recorren los elementos.](https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity)) lo cual vuelve a KNN un algoritmo costoso cuando $$n\rightarrow \infty$$. 

Otros problemas propios del dataset que pueden presentar algunas desventajas son

1.  Cantidad de datos es masiva, por lo que algunos algoritmos se vuelven costosos (Como se vio en KNN)
1.  Densidad de datos: No todos los items tienen información i.e. un usuario por lo general no vota en todas las películas de IMDB, sino que sólo en aquellas que le interesan. 
1.  *Cold start problem*, que se refiere al problema de los requerimientos iniciales de popular una base de datos, debido a la de baja cantidad de usuarios y ratings al inicio de una aplicación, lo que también es válido para nuevos usuarios que no han definido sus preferencias o presentan pocos ratings.

Una forma de abordar el problema es la del filtrado colaborativo basado en **ítems**.


## Item based collaborative filtering

Un

Existen elementos que son cuantificables de manera directa, i.e.

1. Popularidad
1. Género
1. Director
1. Ratings

En cambio hay otros elementos que no, como

1. Título
1. Descripciones

<!-- slide 4/25 -->
Componentes principales

1.  Analizador de contenido
1.  Aprendizaje del perfil de usuario
1.  Filtrado de contenido

## Bag of words
### Representación del contenido: VSM

<!-- slide 6 -->
<!-- Aquí aparece una tabla -->

Un vector de palabras, o *corpus*, para el caso del español por ejemplo, es de aprox $60000$ palabras. 

Un documento puede representarse como un vector donde cada elemento es la frecuencia de cada palabra del corpus, 

$$ v = [f1,f2,...,fn] $$

donde $fi$ es la frecuencia de cada palabra

<!-- TODO: cambiar notaciones  -->

sin embargo conviene escribir las frecuencias de manera ponderada, onde cada peso es la frecuencia dividido por la frecuencia máxima presente en el documento

<!--  slide 9 -->

$$ TF = fi/fMAX $$

Estos pesos se suavizan a través de logaritmos, tales que 

$$ log(TF) $$

<!-- slide 8 -->


## TF-IDF

*  N es la canntidad de documentos en el corpus
*  $n_k$ es la cantidad de documentos donde aparece la palabra
*  El valor de $log(N/n_k)$ tiende a cero cuando la palabra aparece en muchos documentos, por ejemplo artículos, preposiciones, etc.
*  el término TF significa *term frequency* 

## Representación semántica del contenido

## Distancia entre dos elementos

*  Distancia euclidiana
*  Distancia coseno

Para distancias entre dos puntos en el contexto de *bag of words* no resulta muy buena la distancia euclidiana. (*Por qué?*)

Si normalizo todos los vectores, es posible compararlos por el 
ángulo con la distancia coseno.

## OKAPI BM25
<!-- Slide 14 -->

* (k1+1), k1 es una constante que hay que ajustar
* Ld es el largo del documento
* Lave es el largo promedio de todos los documentos
* Ojo con TFq vs TFd, donde q es para la frecuencia del término en la *query* versus el documento

## Técnicas de procesamiento adicionales

Problemas posibles durante una query

* Palabras mal escritas
    * Se normalizan i.e. pasar todo a minúsculas o mayúsculas
* Tokenization: Dividir una oración en unidades, i.e. *tokens*. La tokenización depende de cada implementación.
* Stemming: Tomar la raíz de las palabras. 
    * Porter: Stem de las primeras letras que se repiten.
    * Krovetz: Stem, pero asociada a una palabra existente.
* Lemmatization: No caché.

Ejemplo de tokens en una oración


>*Esta es una __oración__ de ejemplo. Esta __oración__ tiene dos instancias de la palabra __oración__. Tres con esta última*

Cada vez que aparece la palabra *oración* es una instancia o token, habiendo tres instancias distintas en este caso.

## Buscando items similares

Qué pasa si dos palabras so iguales pero tienen distintos significados? 

Se pueden aplicar 
*  Latent semantic Indexing
*  Latent dirichlet allocation

# Ejercicios con Solr


<!-- slide 16 -->
