---
layout: post
title:  "Sistemas recomendadores, parte 3"
date:   2017-11-05 20:01:00 -0600
categories: sysrec spanish
draft: true
---

## SR basados en modelos

### Cuantificación


Existen elementos que son cuantificables de manera directa, i.e.

1. Popularidad
1. Género
1. Director
1. Ratings

En cambio hay otros elementos que no, como

1. Título
1. Descripciones

<!-- slide 4/25 -->
Componentes principales

1.  Analizador de contenido
1.  Aprendizaje del perfil de usuario
1.  Filtrado de contenido

#### Bag of words
#### Representación del contenido: VSM

<!-- slide 6 -->
<!-- Aquí aparece una tabla -->

Un vector de palabras, o *corpus*, para el caso del español por ejemplo, es de aprox $60000$ palabras. 

Un documento puede representarse como un vector donde cada elemento es la frecuencia de cada palabra del corpus, 

$$ v = [f1,f2,...,fn] $$

donde $fi$ es la frecuencia de cada palabra

<!-- TODO: cambiar notaciones  -->

sin embargo conviene escribir las frecuencias de manera ponderada, onde cada peso es la frecuencia dividido por la frecuencia máxima presente en el documento

<!--  slide 9 -->

$$ TF = fi/fMAX $$

Estos pesos se suavizan a través de logaritmos, tales que 

$$ log(TF) $$

<!-- slide 8 -->


#### TF-IDF

*  N es la canntidad de documentos en el corpus
*  $n_k$ es la cantidad de documentos donde aparece la palabra
*  El valor de $log(N/n_k)$ tiende a cero cuando la palabra aparece en muchos documentos, por ejemplo artículos, preposiciones, etc.
*  el término TF significa *term frequency* 

## Representación semántica del contenido

## Distancia entre dos elementos

*  Distancia euclidiana
*  Distancia coseno

Para distancias entre dos puntos en el contexto de *bag of words* no resulta muy buena la distancia euclidiana. (*Por qué?*)

Si normalizo todos los vectores, es posible compararlos por el 
ángulo con la distancia coseno.

## OKAPI BM25
<!-- Slide 14 -->

* (k1+1), k1 es una constante que hay que ajustar
* Ld es el largo del documento
* Lave es el largo promedio de todos los documentos
* Ojo con TFq vs TFd, donde q es para la frecuencia del término en la *query* versus el documento

## Técnicas de procesamiento adicionales

Problemas posibles durante una query

* Palabras mal escritas
    * Se normalizan i.e. pasar todo a minúsculas o mayúsculas
* Tokenization: Dividir una oración en unidades, i.e. *tokens*. La tokenización depende de cada implementación.
* Stemming: Tomar la raíz de las palabras. 
    * Porter: Stem de las primeras letras que se repiten.
    * Krovetz: Stem, pero asociada a una palabra existente.
* Lemmatization: No caché.

Ejemplo de tokens en una oración


>*Esta es una __oración__ de ejemplo. Esta __oración__ tiene dos instancias de la palabra __oración__. Tres con esta última*

Cada vez que aparece la palabra *oración* es una instancia o token, habiendo tres instancias distintas en este caso.

## Buscando items similares

Qué pasa si dos palabras so iguales pero tienen distintos significados? 

Se pueden aplicar 
*  Latent semantic Indexing
*  Latent dirichlet allocation

# Ejercicios con Solr


[^fn]: And the footnote!
