---
layout: post
title:  "Sistemas recomendadores, parte 3"
date:   2017-11-05 20:01:00 -0600
categories: sysrec spanish
published: true
---

En la [parte 2]({{ site.baseurl }}{% post_url 2017-10-28-sysrec-2 %}) se vieron algunas clasificaciones de SR, con algo más de detalle en aquellos basados en memoria y basados en modelos. Otras clasificación que se mencionó es la de aquellos SR basados en contenido

## Sistemas recomendadores basados en contenido

Existen elementos que son cuantificables de manera directa, i.e.

1. Popularidad
1. Género
1. Director
1. Ratings

Al respecto las recomendaciones sobre estos datos pueden abordarse con SR basados en filtrado colaborativo. Se vio  sin embargo que estos SR sufren de algunos problemas debido a la naturaleza poco densa de los datos en el contexto de recomendación y del *new item problem*, entre otros. 

Podemos decir también que hay otros elementos que no son cuanntificables directamente y dependen de otras cosas, por ejemplo el contexto

1. Título de una película
1. Descripciones de un evento

Si existen descripciones suficientes es posible generar recomendaciones a partir del análisis de contenido, lo que abre las posibilidades del uso de otras técnicas tales como el análisis de texto (En python existen dos 
herramientas para esto, [NLTK](www.nltk.org) y [Spacy](https://spacy.io/)), inferencia estadística, entre otras.


<!-- slide 4/25 -->
Los componentes principales de este tipo de SR son

1.  Análisis de contenido
1.  Aprendizaje del perfil de usuario
1.  Filtrado de contenido

{: style="text-align:center"}
![cbrs]( {{ "/assets/img/cbrs.png" | absolute_url }} "cbrs_process")

Estos SR presentan la ventaja de que es más fácil explicar las recomendaciones en función del mismo contenido, sin embargo puede ocurrir lo que se conoce como *filter bubble*, situación en la que las nuevas recomendaciones terminan siendo muy similares a lo ya consumido. Por ejemplo, si me gusta Harry Potter, puede ocurrir que el SR sólo pueda recomendarme libros de Harry Potter y no de la narrativa fantástica en general.


## Análisis de contenido

El aspecto más importante de este tipo de SR es su dependencia de la representación del contenido.. De manera más general esto se conoce como *information retrieval*, y tiene que ver en cómo se extrae información de los datos. 

En ese sentido, la forma más inmediata es la de analizar el texto en las descripciones de los ítems que forman parte del dataset

### Análisis de texto
#### Representación vectorial

La primera representación que se hace del texto es a través de lo que se conoce como *bag of words*, donde todas las palabras se consideran como instancias de cada palabra, en forma de repeticiones. 
Esto a su vez permite otras representaciones, una es VSM o *Vector space model*. Básicamente se trata de vectorizar términos en función de su aparición en una familia de documentos como por ejemplo

{: style="text-align:center"}
![vsm1]( {{ "/assets/img/vsm1.png" | absolute_url }} "vsm1")

Un documento entonces puede representarse como un vector donde cada elemento es la frecuencia de cada palabra del corpus en el documento, 

$$ v = [f_{1},f_{2},...,f_{n}] $$

donde $$f_{i}$$ es la frecuencia de cada palabra. Un vector de palabras, o *corpus*, para el caso del español por ejemplo, es de aproximadamente 60000 palabras. Estos vectores tienen propiedades geométricas que permiten establecer comparaciones entre ellos.

La frecuencia de las palabras por sí sola en un documento no ayuda necesariamente a establecer similitudes, por ejemplo es muy posible encontrar muchas veces en un documento las palabras *él, ella, qué, etcétera*, lo que no ayudan mucho. Dos documentos no son necesariamente iguales si tienen relativamente la misma cantidad de veces la palabra *qué*.

Es por esto que se desarrollan métodos de regularización, el primero es normalizar la frecuencia de los términos, a través de lo que se conoce como TF o *term frequency*

$$ \mbox{TF}(\mbox{palabra},\mbox{documento}) = \frac{\mbox{veces que sale la palabra en el documento}}{\mbox{cantidad máxima de veces que sale la palabra en todos los documentos}} $$

Además, si se considera nuevamente como ejemplo la palabra *qué*, esta no es más importante en un documento si aparece 10000 veces, versus otras palabras que aparecen con menor frecuencia. Para incorporar esto se usan logaritmos

$$
\left\{\begin{aligned}
& 1+\log_{10} \mbox{TF}_{p,d} & \mbox{TF}_{p,d} \ge 1\\
& 0 & \mbox{TF}_{p,d} = 0\\
\end{aligned}
\right.$$

<!-- slide 8 -->
Finalmente combinando ambos elementos, es posible obtener una representación estable a través de TF-IDF 


$$ \mbox{TF-IDF}_{p,d} =  \mbox{TF}_{p,d} \times \log \frac{N}{n_{p}}
 $$

Algunas observaciones 
*  N es la cantidad de documentos del dataset
*  $$n_{p}$$ es la cantidad de documentos donde aparece la palabra
*  El valor de $$\log(N/n_{p})$$ tiende a cero cuando la palabra aparece en muchos documentos (por ejemplo artículos, preposiciones, etc.)

#### Representación semántica del contenido

El texto visto como *bag of words* pierde sentido contextual, es decir la vectorización no incorpora semántica en las recomendaciones. El contexto en sí mismo añade una nueva capa al análisis de texto, y entre las formas de incorporarlo  se pueden usar *ontologías*. La [web semántica](https://en.wikipedia.org/wiki/Semantic_Web) intenta modelar los contenidos de internet a través de estructuras ontológicas estandarizadas llamadas [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)

#### Métricas de similitud

La vectorización de documentos permite operaciones geométricas y en consecuencia es posible establecer métricas de distancia entre ellos. Las más conocidas ya se han visto y corresponden a

*  Distancia euclidiana
*  Distancia coseno

Sin embargo, los *corpus* como se vio son de alta dimensionalidad, (60000 palabras para el español, por ejemplo) donde la maldición de la dimensionalidad se presenta nuevamente como un problema. Al respecto, si se normalizan todos los vectores, es posible obtener mejores resultados con la distancia coseno.

Las métricas posibles incluyen [OKAPI BM25](https://dl.acm.org/citation.cfm?doid=1639714.1639757) entre otras.
<!-- Slide 14 -->

<!-- * (k1+1), k1 es una constante que hay que ajustar -->
<!-- * Ld es el largo del documento -->
<!-- * Lave es el largo promedio de todos los documentos -->
<!-- * Ojo con TFq vs TFd, donde q es para la frecuencia del término en la *query* versus el documento -->


Puede ocurrir que dos palabras son iguales pero tienen distintos significados, lo que complica las comparaciones. Al respecto existen técnicas como

*  *Latent semantic Indexing*
*  *Latent dirichlet allocation*


#### Técnicas de procesamiento adicionales

El análisis de texto presenta otro tipo de problemas, por ejemplo palabras mal escritas, calidad del texto, etcétera. Algunas técnicas auxiliares usadas en el procesamiento de texto son

*  Normalización i.e. pasar todo a minúsculas o mayúsculas
*  Tokenizatión: Dividir una oración en unidades, i.e. *tokens*. La tokenización depende de cada implementación.
*  *Stemming*: Tomar la raíz o *stem* de las palabras que compartan una semántica i.e *auto* en *Automovilismo* y *Autopista* 
  *  *Porter*: Stem de las primeras letras que se repiten.
  *  *Krovetz*: Stem, pero asociada a una palabra existente.
*  Lemmatization: Una variante de *stemming* que incorpora el análisis morfológico del texto. Ver más información [aquí](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)


## A continuación

*  Sistemas recomendadores híbridos
*  Métricas de calidad en sistemas recomendadores 
