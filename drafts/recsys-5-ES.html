<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Sistemas recomendadores, parte 5 | On the shoulders of giants
</title>
  <link rel="canonical" href="/drafts/recsys-5-ES.html">

    <link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/fontawesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/monokai.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">

  
  <meta name="description" content="Quinta parte de sistemas recomendadores">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="/">
        <img class="img-fluid rounded" src=/images/profile.png width=400 height=400 alt="On the shoulders of giants">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="/">On the shoulders of giants</a></h1>
      <p class="text-muted">Diego Quintana's blog</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="/pages/about.html">About</a></li>
            <li class="list-inline-item"><a href="/pages/now-EN.html">Now</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/diegoquintanav" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/diego-quintana-valenzuela/" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-stack-overflow" href="https://stackoverflow.com/users/5819113/bluesmonk" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-envelope" href="/mailto:daquintanav@gmail.com" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-spotify" href="https://open.spotify.com/user/11102438968?si=a22574d2e0214ba8" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Sistemas recomendadores, parte 5
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2017-11-15T18:01:00-06:00">
          <i class="fas fa-clock"></i>
          Wed 15 November 2017
        </li>
        <li class="list-inline-item">
          <i class="fas fa-folder-open"></i>
          <a href="/category/recommender-systems.html">Recommender Systems</a>
        </li>
          <li class="list-inline-item">
            <i class="fas fa-tag"></i>
              <a href="/tag/recsys.html">#recsys</a>          </li>
      </ul>
    </header>
    <div class="content">
      <!-- entry 5, clase al 15.11 -->

<h1><em>Previously</em></h1>
<p>En el <a href="/drafts/recsys-4.html">post anterior</a> se hizo una revisión de los Sistemas recomendadores híbridos y se revisaron algunas métricas usadas en su evaluación. Lo que queda como última parte de esta serie, es hablar sobre Máquinas de factorización. Las máquinas de factorización pueden verse como máquinas de vectores de soporte (SVM, <em>support vector machines</em>), utilizando con un kernel polinomial. </p>
<p>Pero antes, ¿Qué es una máquina de vectores de soporte?</p>
<h2>Máquinas de vectores de soporte</h2>
<p>Es uno de los clasificadores clásicos usados en machine learning. Dado que no es la idea profundizar en este algoritmo, se podría resumir en lo siguiente</p>
<p>Dadas dos clases <strong>A</strong> y <strong>B</strong>, una SVM es un clasificador $\mathcal{C}$ que encuentra un plano de decisión $h$, tal que la distancia de tal plano a dos vectores llamados de soporte sea máxima. Estos vectores de soporte son obtenidos a través de un proceso de optimización, y los planos de decisión pueden extenderse a otras superficies más complejas a través de lo que se conoce como <em>the kernel trick</em> (Explicado muy bien <a href="http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html">aquí</a>).</p>
<p><img alt="svm1" src="https://ml.berkeley.edu/blog/assets/tutorials/2/image_2.png"></p>
<p>Un <em>kernel</em> resuelve un problema donde para algún conjunto de datos, no existe un plano de decisión $h$ o en otras palabras las clases no son separables. El kernel actúa como una transformación que en ciertas condiciones, permite operar sobre una representación de $\mathcal{C}$ donde sí existe la separación $h$.</p>
<p><img alt="kernel1" src="/images/data_2d_to_3d.png"></p>
<p>Un kernel se escribe matemáticamente como</p>
<p>$$
K: \mathbb{R}^M \times \mathbb{R}^M \rightarrow \mathbb{R}^N
$$</p>
<p>Tal que</p>
<p>$$
K(\vec{x}<em j>{i},\vec{x}</em>) = \langle { \phi(\vec{x}<em j>{i}),\phi(\vec{x}</em>) } \rangle
$$</p>
<p>donde $\langle .,. \rangle$ es el producto punto, $M &gt; N$ y $\phi$ es la transformación del espacio de características. La gracia de este <em>truco</em> es que permite operar sobre espacios de mayor dimensionalidad (donde sí existe $h$) pero dentro del espacio de menor dimensionalidad (lo que podría ser muy costoso computacionalmente de lo contrario).</p>
<p>Sobre <em>kernels</em> hay algunos muy populares, por ejemplo</p>
<ul>
<li>Polinomial</li>
<li><em>Radial Basis Function</em></li>
<li>Sigmoidal</li>
</ul>
<p>Una definición más completa sobre SVM puede leerse en <a href="https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/">esta página</a>.</p>
<h2>Máquinas de factorización</h2>
<p>Las SVM presentan problemas sobre datasets muy poco densos, problema que es abordado por las máquinas de factorización, las que incorporan la interacción entre distintas variables en el dataset. Para dos interacciones, <a href="https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf">Rendle, 2010</a> define el modelo de una FM como</p>
<p>$$
\hat{y}(x) = w_{0} + \sum_{i=1}^{n} w_{i} x_{i} + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle {v_{i},v_{j}} \rangle  x_{i} x_{j}
$$</p>
<p>Reconocemos en $\langle {v_{i},v_{j}} \rangle$ un <em>kernel</em> polinomial de la forma</p>
<p>$$
\langle {v_{i},v_{j}} \rangle = \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f}
$$</p>
<p>En este caso el <em>kernel trick</em> permite reducir la complejidad final lineal al orden de $\mathcal{O}(kn)$, sin aumentar la complejidad de la función objetivo. Para datasets menos densos como lo son aquellos usados para recomendación, las máquinas de factorización operan mucho mejor que otros algoritmos, aunque sus aplicaciones no se limitan sólo a <em>SR</em>.</p>
<h2>Código</h2>
<ul>
<li>Steffen Rendle creó <a href="http://www.libfm.org/">LibFM</a>, una librería para implementaciones de FM en C++ disponible en github.</li>
<li>Existen otras implementaciones en <em>Spark</em> para aprovechar la parelización en <a href="https://github.com/blebreton/spark-FM-parallelSGD">este repositorio de Github</a></li>
</ul>
<h2>The end</h2>
<p>No quedan más partes, ¡Con esto cerramos esta serie!</p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>

</body>

</html>