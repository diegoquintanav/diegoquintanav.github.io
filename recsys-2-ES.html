<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Sistemas recomendadores, parte 2 | On the shoulders of giants
</title>
  <link rel="canonical" href="/recsys-2-ES.html">

    <link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/monokai.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">

  
  <meta name="description" content="Segunda parte de sistemas recomendadores">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="/">
        <img class="img-fluid rounded" src=/images/profile.png width=400 height=400 alt="On the shoulders of giants">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="/">On the shoulders of giants</a></h1>
      <p class="text-muted">Diego Quintana's blog</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="/pages/about.html">About</a></li>
            <li class="list-inline-item"><a href="/pages/contact-EN.html">Contact</a></li>
            <li class="list-inline-item"><a href="/pages/now-EN.html">Now</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/diegoquintanav" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-linkedin" href="https://www.linkedin.com/in/diego-quintana-valenzuela/" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-stack-overflow" href="https://stackoverflow.com/users/5819113/bluesmonk" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-envelope" href="mailto:daquintanav@gmail.com" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Sistemas recomendadores, parte 2
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2017-10-28T20:01:00-06:00">
          <i class="fa fa-clock-o"></i>
          sáb 28 octubre 2017
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="/category/recommender-systems.html">recommender systems</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="/tag/recommender-systems.html">#recommender systems</a>,               <a href="/tag/recsys.html">#recsys</a>          </li>
      </ul>
    </header>
    <div class="content">
      <h1><em>Previously</em></h1>
<p>En la <a href="/recsys-1-ES.html">parte 1</a> se vieron varios aspectos elementales
de los sistemas recomendadores. El concepto del error, Un modelo inicial de una
recomendación, y la necesidad que cubren estos sistemas. 
Mencionamos <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.423.5258&amp;rep=rep1&amp;type=pdf">este paper</a> 
con deficiones más rigurosas. La idea ahora es introducir algunos conceptos generales
sobre los tipos de sistemas recomendadores.</p>
<p>Podemos agrupar clasificadores según sus características. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.702.4429&amp;rep=rep1&amp;type=pdf">Este paper</a> presenta algunas directrices, las que serán reflejadas aquí de manera 
menos rigurosa.</p>
<h1>Tipos de recomendadores</h1>
<h2>Según el agente que <em>controla</em> el <em>SR</em></h2>
<p>El agente se refiere aquí a quién <em>entrena</em> al recomendador. Al respecto se indican</p>
<ol>
<li>Sistemas customizados</li>
<li>Sistemas personalizados</li>
</ol>
<p>El primero es aquel en que <em>el usuario</em> modifica su perfil manualmente en base a sus preferencias, mientras que en el segundo es la implementación del <em>SR</em> la 
que crea y actualiza el perfil para cada usuario, con control explícito mínimo del usuario, e incluso nulo en algunas ocasiones.</p>
<h2>Según la forma que los modelos aprenden</h2>
<p>Esta clasificación es más primitiva, y da lugar a</p>
<ul>
<li>Sistemas <em>basados en memoria</em></li>
<li>Sistemas <em>basados en modelos</em></li>
</ul>
<p>Los primeros son aquellos que <strong>usan datos</strong> i.e clicks, ratings, votos, etc. y establecen una métrica de similitud entre usuarios o ítems. Estas métricas pueden ser de diversos tipos, entre las cuales conviene mencionar</p>
<ul>
<li>Similitud coseno</li>
<li>Correlación de pearson</li>
<li>Distancia euclidiana</li>
</ul>
<p>Estas recomendaciones permiten establecer distancias entre personas y/o ítems. Si consideramos una matriz <span class="math">\(M\)</span> donde los usuarios son filas y los ítems columnas, los <em>SR</em> basados en memoria establecen similitud entre dos vectores, pudiendo ser</p>
<ul>
<li>Entre usuarios (dos o más filas) </li>
<li>Entre ítems (dos o más columnas)</li>
<li>Entre un usuario y un ítem</li>
</ul>
<p>Este tipo de modelos presentan algunos problemas debido a las características de <span class="math">\(M\)</span>, como por ejemplo</p>
<ol>
<li>El tamaño de <span class="math">\(M\)</span> es muy grande: Si la cantidad de datos es masiva, algunos algoritmos que operan sobre todo <span class="math">\(M\)</span> se vuelven costosos (Como se verá en KNN más adelante)</li>
<li>Densidad de datos: No todos los items o usuarios tienen información i.e. un usuario por lo general no vota en todas las películas de IMDB, sino que sólo en aquellas que le interesan. Esto genera vectores más dispersos o con más elementos vacíos.</li>
<li>El problema de la partida en frío o <em>Cold start</em>. Cuando un recomendador comienza
no tiene información de ratings, por lo que no puede recomendar nada. El problema
implica entonces el cómo se completa inicialmente esta información. Esto también es válido para nuevos usuarios que no han definido sus preferencias o presentan pocos ratings.</li>
</ol>
<p>Por el otro lado, los <strong><em>SR</em> basados en modelos</strong> tratan de completar esta matriz con probabilidades de que un usuario valore positivamente un ítem que no había encontrado antes. Para esto se apoyan en algoritmos de aprendizaje automático o <em>machine learning</em>.</p>
<h3>Métricas de similitud</h3>
<h4>Similitud coseno</h4>
<p>Si se consideran dos vectores de dimensionalidad <span class="math">\(m\)</span>, se define la similitud entre ambos a través del cálculo del <em>coseno</em> del ángulo entre ambos vectores. Formalmente, Si consideramos la matriz <span class="math">\(M\)</span> de <span class="math">\(m\)</span> usuarios y <span class="math">\(n\)</span> items, la similitud entre dos elementos está dada por</p>
<div class="math">$$sim(i,j) = \cos (\vec{i},\vec{j}) = \frac{\vec{i} \cdot \vec{j}}{ { \lVert \vec{i} \rVert }_{2} \ast { \lVert \vec{j} \rVert }_{2} }$$</div>
<h4>Correlación de Pearson</h4>
<p>Una métrica de similitud se obtiene a través de la correlación de Pearson. Esta permite aislar los casos donde ocurra <em>co-validación</em> i.e. aquellos casos en que el usuario ha validado ítems <span class="math">\(i\)</span> y <span class="math">\(j\)</span> (ver imagen, obtenida del paper).</p>
<div class="math">$$sim(i,j) = \frac{ \sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{i})(r_{u,j}-\bar{r}_{j}) }{ \sqrt{\sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{i})^2} \sqrt{\sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{j})^2} }$$</div>
<p>Donde <span class="math">\(r_{u,j}\)</span> es el rating del usuario <span class="math">\(u\)</span> sobre el ítem <span class="math">\(j\)</span> y <span class="math">\(\bar{r}_{j}\)</span> es el <em>rating</em> promedio sobre el otro ítem.</p>
<p><img alt="IB-CF1" src="/images/CF-IB_1.png" title="IB-CF" /></p>
<h4>Similitud coseno ajustada</h4>
<p>Se obtiene incorporando el promedio <span class="math">\(\bar{r}_{u}\)</span> de votos de cada usuario en el cálculo de similitud, es decir</p>
<div class="math">$$sim(i,j) = \frac{ \sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{u})(r_{u,j}-\bar{r}_{u}) }{ \sqrt{\sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{u})^2} \sqrt{\sum_{u \in \mathcal{U}}(r_{u,i}-\bar{r}_{u})^2} }$$</div>
<p>De los tres métodos indicados, es esta noción de similitud la que obtiene mejores resultados</p>
<p><img alt="IB-CF2" src="/images/similarities.png" title="Similarities comparison" /></p>
<h2>Otra propuesta de clasificación</h2>
<p>También es posible dividir los <em>SR</em> en cuatro grandes grupos, con base en</p>
<ul>
<li>El tipo de datos de entrada que utilizan</li>
<li>La forma en que construyen perfiles de usuario</li>
<li>Los métodos algorítmicos usados para producir recomendaciones</li>
</ul>
<p>Estos son</p>
<h3>Basados en reglas (<em>Rule based</em>)</h3>
<p>Las reglas son definidas manual y explícitamente en función de los objetivos del 
<em>SR</em>, por lo que tienden a ser más rígidos y a depender más de conocimiento 
experto. Esto ocurre mucho en sitios de <em>e-commerce</em>, donde los <em>SR</em> reflejan en 
gran medida los intereses de la compañía.</p>
<p>La gran desventaja de estos <em>SR</em> es que dependen de reglas explícitas, y que 
alimentan los perfiles en base a la ingesta manual de intereses del usuario. Todo 
esto incluye un sesgo que entorpece el dinamismo del <em>SR</em> ante nuevos escenarios.</p>
<h3>Basados en contenido (<em>Content based</em>)</h3>
<p>La principal característica de este <em>SR</em> es que sus recomendaciones se basan en los 
ítems que el usuario ha demostrado interés explícito anteriormente (ejemplo <em>"Add 
to my wishlist</em>"). </p>
<p>Estos recomendadores usan métricas de similitud a través de algoritmos de clasificación, <em>clustering</em> o análisis de texto. Un ejemplo es un recomendador que se basa de las descripciones de productos.</p>
<p>Uno de los contras de estos <em>SR</em> es que dependen de los hábitos y gustos de un 
usuario para poder recomendar nuevos ítems, lo que produce una burbuja. </p>
<p>Esto se traduce en los siguientes problemas de recomendación</p>
<ul>
<li><em>¿Cómo puedo recomendar productos nuevos a alguien si no sabe que existen?</em></li>
<li><em>¿Cómo decido qué nuevos productos presentarle primero?</em> </li>
</ul>
<p>En estos casos los ítems más novedosos son la mejor elección.</p>
<h3>Filtrado colaborativo (<em>User based</em>)</h3>
<p>Apuntan a resolver los problemas presentes en los dos <em>SR</em> anteriores, utilizando 
los datos como clicks y ratings de otros usuarios en la <em>vecindad</em> del usuario 
usando el <em>SR</em>.</p>
<p>Tradicionalmente, estos sistemas se dicen <em>Basados en memoria</em> (<em>Memory based</em>, un 
término que se considera a veces obsoleto, y que a veces es reemplazado por <em>User 
based</em> o <em>Item based</em>), o dicho de otra forma, necesitan tener la información de 
los intereses de <em>todos</em> los usuarios para poder emitir una recomendación. Un 
ejemplo de un algoritmo muy usado en <em>clustering</em> esto es el algoritmo de <span class="math">\(k\)</span> 
vecinos cercanos o <strong>KNN</strong> (<em>K Nearest Neighbors</em>).</p>
<p>Un problema que presentan estos <em>SR</em> es que operan sobre datasets por lo general 
con poca información. Por ejemplo digamos que un usuario puede haberse manifestado 
sobre dos productos en un conjunto de 1000 productos. El vector de ratings para 
este usuario sería de la forma</p>
<div class="math">$$x_{i} = {0,...,r_{34},r_{35},...,0}$$</div>
<p>donde <span class="math">\(r_{34}\)</span> y <span class="math">\(r_{35}\)</span> serían dos votaciones distintas de cero. Se tiene 
entonces que existen 998 ceros que no aportan información alguna, lo que se 
transforma en recomendaciones de mala calidad al comienzo de la creación de un 
perfil de usuario. Si consideramos que constantemente se agregan nuevos ítems, este 
dataset se diluye cada vez más.</p>
<p>Otro problema que existen en este tipo de <em>SR</em> es el <em>new item problem</em>. Cuando un 
nuevo item es añadido al pool de opciones, no existe en ningún perfil de usuario y 
el <em>SR</em> no puede recomendárselo a nadie.</p>
<p>Finalmente, uno de los pincipales problemas que presentan estos algoritmos es que 
no son escalables. Por ejemplo, para el algoritmo KNN, la formación de <em>clusters</em> obtiene un modelo utilizando todo el conjunto de datos a la vez, lo cual presenta
problemas si pensamos que la generación de nuevos ratings y productos puede ocurrir con mucha frecuencia, incluso a diario.</p>
<h4>Clasificación de recomendadores basados en filtrado colaborativo</h4>
<p>El filtrado colaborativo puede dividirse en dos grandes grupos, dependiendo sobre cuál es la entidad usada para establecer patrones o <em>clusters</em></p>
<ul>
<li>filtrado colaborativo basado en usuarios (UB-CF)</li>
<li>filtrado colaborativo basado en ítems (IB-CF)</li>
</ul>
<p>Si se considera la matriz <span class="math">\(M\)</span> indicada anteriormente, donde las filas son usuarios 
y las columnas ítems, entonces una diferencia fundamental entre estos grupos es el 
eje sobre el cual se establece la similitud, siendo las filas para el primer caso y 
las columnas para el segundo.</p>
<p>La importancia de esto es que cada par en el conjunto a calcular corresponde a 
usuarios distintos. Esto tiene algunas desventajas, como la pérdida de escala entre 
usuarios i.e. no todos los usarios evalúan el mismo ítem de la misma manera, y este 
factor se diluye entre todos los usuarios. Una forma de resolver es con la 
<em>similitud coseno ajustada</em>.</p>
<p>Al respecto existen varios tipos de algoritmos basados en el filtrado colaborativo, y dos ejemplos pueden ser</p>
<ul>
<li><a href="https://arxiv.org/abs/cs/0702144">Slope One</a></li>
<li>FunkSVD, <a href="http://sifter.org/simon/journal/20061211.html">ganador del premio Netflix</a></li>
</ul>
<h5>User Based Collaborative Filtering (UB-CF)</h5>
<p>Se puede resumir como</p>
<ol>
<li>La predicción del rating sobre un ítem <span class="math">\(i\)</span> mejora mientras más vecinos <span class="math">\(k\)</span> de un 
   <em>pool</em> de <span class="math">\(n\)</span> elementos de dimensión <span class="math">\(d\)</span> se consideren en el cálculo de similitud.</li>
<li>La complejidad de calcular la distancia a un ejemplo es de <span class="math">\(\mathcal{O}(d)\)</span></li>
<li>La complejidad de calcular la distancia a <span class="math">\(n\)</span> ejemplos es de <span class="math">\(\mathcal{O}(dn)\)</span></li>
<li>Una vez calculadas las <span class="math">\(n\)</span> distancias, la complejidad de recorrer los <span class="math">\(k\)</span> 
   vecinos más cercanos es de <span class="math">\(\mathcal{O}(nk)\)</span></li>
<li>El tiempo total entonces es de <span class="math">\(\mathcal{O}(nd+nk)\)</span>, (o <span class="math">\(\mathcal{O}(ndk)\)</span> 
   <a href="https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity">en función de cómo se recorren los elementos.</a>) lo cual vuelve a KNN un algoritmo costoso cuando <span class="math">\(n\)</span> es lo suficientemente grande.</li>
</ol>
<p>Un <em>SR</em> alternativo al <em>User Based</em> (UB-CF) es la del filtrado colaborativo basado en <em>ítems</em>, o <em>Item based collaborative filtering (IB-CF)</em></p>
<h5>Item Based collaborative filtering (IB-CF)</h5>
<p>Nacen de la necesidad de generar recomendaciones más rápidas sobre datasets masivos,
para los cuales los algoritmos de tipo UB-CF no escalan bien. </p>
<p>Estas técnicas analizan primero la matriz de usuarios e ítems <span class="math">\(M\)</span> para identificar 
relaciones entre diferentes ítems, y a través de éstas computar calcular de manera 
indirecta nuevas relaciones con los usuarios.</p>
<p>Dicho de otra forma, estos algoritmos exploran las relaciones entre ítems similares,
en vez de realizarla sobre usuarios similares. De esta manera las recomendaciones 
nuevas se logran encontrando ítems similares a otros que el usuario <span class="math">\(u\)</span> ha 
calificado positivamente. Dado que las relaciones entre ítems es <em>relativamente</em> 
estática (a diferencia de los gustos de los usuarios, que son más dinámicos), estas 
recomendaciones tienden a ser más estables en el tiempo.</p>
<p>De la misma manera que en el caso de UB-CF, algunas métricas de similitud pueden ser la <em>correlación entre ítems</em>, o <em>similitud coseno</em>. <a href="http://files.grouplens.org/papers/www10_sarwar.pdf">Este paper</a> hace una revisión de los métodos indicados, y concluye que este tipo de <em>SR</em> poseen un rendimiento superior, además de proveer mejores recomendaciones que su compañero UB-CF.</p>
<h3>Sistemas híbridos</h3>
<p>Los <em>SR</em> basados en contenido y en filtrado colaborativo presentan sus propios problemas. Los sistemas híbridos apuntan a resolver estas limitaciones a través de
combinaciones de otros <em>SR</em></p>
<h2>A continuación</h2>
<p>En la <a href="/recsys-3-ES.html">parte 3</a> se verán los sistemas de recomendación basados en contenido con mayor detalle.</p>
<h2>Anexo: KNN y la maldición de la dimensionalidad</h2>
<p>(o en inglés, <em>curse of dimensionality</em>)</p>
<h3>K <em>Nearest neighbors</em> (KNN)</h3>
<p>Este algoritmo crea modelos de grupos o <em>clusters</em> de usuarios o ítem que guardan 
relación entre sí de acuerdo a una métrica preestablecida. La idea de KNN en el 
contexto de recomendación es, en términos simples:</p>
<blockquote>
<p>Para un subconjunto de usuarios <span class="math">\(\mathcal{U} \in U\)</span>, encuentra los ratings para
un item <span class="math">\(i \in I\)</span> sobre los primeros <span class="math">\(k\)</span> usuarios (vecinos en este contexto) que 
sean <em>objetivamente</em> similares a un usuario <span class="math">\(u \in \mathcal{U}\)</span>\$.</p>
</blockquote>
<p>¿Pero cómo se logra establecer similitud <em>objetivamente</em>?. Se mide la distancia 
entre dos objetos y vemos si la distancia es lo suficientemente pequeña. Esto se 
puede conseguir generalmente a través de una distancia euclidiana o coseno. <a href="https://blog.dominodatalab.com/recommender-systems-collaborative-filtering/">Esta página tiene algunas definiciones sobre esto, y presenta algunos ejemplos usando Python</a>.</p>
<p>Con una definición <em>más rigurosa</em>, decimos que el <em>rating</em> de un 
usuario sobre un item a predecir, es la suma ponderada de los <em>ratings</em> de otros 
<span class="math">\(k\)</span> usuarios sobre el mismo ítem que comparten similitud.</p>
<div class="math">$$R(u,i) = \bar{r}_u + \alpha \sum_{j=1}^{k} w(u,j)(r_{j,i}-\bar{r}_j)$$</div>
<p>donde 
- <span class="math">\(\alpha\)</span> es un elemento de <em>regularización</em>
- <span class="math">\(\bar{r}_u\)</span> es el promedio de votos del usuario <span class="math">\(u\)</span>
- <span class="math">\((r_{j,i}-\bar{r}_j)\)</span> es el voto de cada usuario o vecino sobre el ítem <span class="math">\(i\)</span> menos el promedio de sus votos
- <span class="math">\(w(u,j)\)</span> es una función que asigna un peso a cada una de estas valoraciones.</p>
<h2>La maldición de la dimensionalidad (o <em>Curse of dimensionality</em>)</h2>
<p><img alt="curse-of-dimenzionality" src="/images/curse-of-dimensionality.png" title="omgsocursed" /></p>
<p>(Kudos al <a href="https://erikbern.com/2015/10/20/nearest-neighbors-and-vector-models-epilogue-curse-of-dimensionality.html">maravilloso ser humano que hizo la imagen.</a>)</p>
<p>Básicamente se trata de lo siguiente: Mientras más características tengan los 
vectores usados en la modelación de un ítem o recomendación, la distancia entre dos 
elementos <strong>pierde significancia</strong>.</p>
<p>En términos de un CSV o un <em>dataframe</em>, el número de <em>columnas</em> que tiene afecta las medidas entre 
las <em>filas</em>. </p>
<p>Suponiendo un conjunto <span class="math">\(X\)</span> de datos <span class="math">\(x_i\)</span>, cada uno con <span class="math">\(d\)</span> dimensiones, se tiene que la <strong>proporción</strong> entre la distancia de cualquier punto hacia el centroide de <span class="math">\(X\)</span> (o donde se concentra la mayor cantidad de puntos) y otro punto dentro del mismo espacio se diluye a <span class="math">\(0\)</span> <em>en el infinito</em>.</p>
<p>Dicho de otra forma, ambas distancias se vuelven <em>relativamente similares</em>.</p>
<p><em>The curse of dimensionality</em> es un tema recurrente en los <em>SR</em>, y me limitaré a recorrer algunas lecturas muy completas sobre el tema:</p>
<ul>
<li><a href="https://stats.stackexchange.com/questions/169156/explain-curse-of-dimensionality-to-a-child"><em>Explain Like I am Five Years Old</em></a> tiene una versión muy simple del problema. En galletas.</li>
</ul>
<blockquote>
<p>Imagina que debes elegir qué galletas te gustan de un camión de galletas, en 
base al sabor. Asumiendo que hay cuatro tipos de sabores (dulce, salado, ácido, 
amargo), bastaría con encontrar 4 tipos de galletas diferentes para saber cuál te 
gusta más. Si a esto añadimos además una característica de color y asumiendo que 
sólo hay cuatro colores, entonces tienes que comer 4x4 tipos de galletas 
distintas. Asumiendo que la descripción de galleta se hace cada vez más compleja, y ahora incluye el olor de la galleta. Luego la marca.
A medida que las características de la galleta a considerar aumenta, resulta cada vez más difícil saber qué galletas te gustan, y probablemente sufras 
un dolor de estómago antes de poder decidir.</p>
</blockquote>
<ul>
<li><a href="https://math.stackexchange.com/questions/346775/confusion-related-to-curse-of-dimensionality-in-k-nearest-neighbor">Este post</a> también es una discusión interesante sobre el tema.</li>
<li><a href="https://erikbern.com/2015/10/20/nearest-neighbors-and-vector-models-epilogue-curse-of-dimensionality.html">Este blog</a> tiene más información aún ~~además de ahorrarme el tiempo de hacer mi propia imagen~~.</li>
<li><a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">Este otro post</a> presenta una explicación más detallada, <em>que también incluye perritos.</em></li>
</ul>
<p><img alt="curse-of-dimenzinality" src="/images/3Dproblem_separated.png" title="omgin3D" /></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>